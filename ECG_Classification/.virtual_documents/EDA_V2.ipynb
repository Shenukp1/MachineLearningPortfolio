





import random

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pycatch22
import seaborn as sns
from imblearn.over_sampling import SMOTE

# Scipy related imports
from scipy.fft import rfft, rfftfreq
from scipy.signal import find_peaks
from scipy.stats import kurtosis, skew, zscore

# Sklearn related imports
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.manifold import TSNE
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree





FS = 125
LABEL_COL = 187
RANDOM_SEED = 42
CLASSIFICATION_COL = 186








# Paths to data ECG_Classification/.data/mitbih_train.csv
normal_train_path = "./.data/ptbdb_normal.csv"
abnormal_train_path = "./.data/ptbdb_abnormal.csv"
normal_df = pd.read_csv(normal_train_path, header=None)
abnormal_df = pd.read_csv(abnormal_train_path, header=None)
df = pd.concat([normal_df, abnormal_df], axis=0)


normal_df.shape, abnormal_df.shape


normal_df.head()


classes_freq = normal_df.iloc[:, -1].value_counts()
sorted_classes_freq = classes_freq.sort_index()
sorted_classes_freq


classes_freq = abnormal_df.iloc[:, -1].value_counts()
sorted_classes_freq = classes_freq.sort_index()
sorted_classes_freq








assert normal_df.isna().sum().sum() == 0
print("No missing values in normal data!")


assert abnormal_df.isna().sum().sum() == 0
print("No missing value in abnormal data!")


print("Normal Data Column Types:")
normal_df.dtypes


print("Abnormal Data Column Types:")
abnormal_df.dtypes


# Since all the columns are numerical, we will change them all into float32
normal_df = normal_df.astype("float32")
normal_df.dtypes


abnormal_df = abnormal_df.astype("float32")
abnormal_df.dtypes








def visualize_differences_with_line_graph(
    normal: pd.DataFrame, abnormal: pd.DataFrame
) -> None:
    """Visualize differences within group and between groups.

    The signals are chosen at random.

    Args:
      normal : pd.DataFrame
        Dataframe that contains the normal ECG signal of a heart.
      abnormal : pd.DataFrame
        Dataframe that contains the abnormal ECG signal of a heart.

    Returns:
     Two plots, one for abnormal and the other for normal. Each
     containing two signals.
    """
    two_rand_numbers = 2
    # Tell us which signal we are choosing
    first_within_signal = 0
    second_within_signal = 1
    first_between_signal = 0
    second_betwee_signal = 1

    # Randomly chooses a signal
    choose_rand_normal_signal = np.random.randint(0, len(normal), two_rand_numbers)
    choose_rand_abnormal_signal = np.random.randint(0, len(abnormal), two_rand_numbers)
    # Plotting
    first_plot = 1
    second_plot = 2
    fig, axes = plt.subplots(first_plot, second_plot, sharey=True, figsize=(10, 4))

    # Normal signal plotting
    axes[first_between_signal].plot(
        normal.iloc[choose_rand_normal_signal[first_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_normal_signal[first_within_signal]} ",
    )
    # Second normal signal
    axes[first_between_signal].plot(
        normal.iloc[choose_rand_normal_signal[second_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_normal_signal[second_within_signal]} ",
    )
    axes[first_between_signal].legend(
        shadow=True, frameon=True, facecolor="inherit", loc=1, fontsize=9
    )
    axes[first_between_signal].set_title("Normal")

    # Only need to set one y-axis title because they both have the same titles
    axes[first_between_signal].set_ylabel("Amplitude (mV)")

    # Abnormal Signal plotting
    axes[second_betwee_signal].plot(
        abnormal.iloc[choose_rand_abnormal_signal[first_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_abnormal_signal[first_within_signal]} ",
    )
    # Second abnormal signal
    axes[second_betwee_signal].plot(
        abnormal.iloc[choose_rand_abnormal_signal[second_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_abnormal_signal[second_within_signal]} ",
    )
    axes[second_betwee_signal].legend(
        frameon=True, facecolor="inherit", loc=1, fontsize=9
    )
    axes[second_betwee_signal].set_title("Abnormal")
    plt.tight_layout()
    plt.show()


visualize_differences_with_line_graph(abnormal=abnormal_df, normal=normal_df)








def visualize_informative_regions(
    normal: pd.DataFrame,
    abnormal: pd.DataFrame,
    time_points: list[int],
) -> None:
    """Visualizes variation/differences in selected time points.

    Args:
      normal : pd.DataFrame
        Dataframe that contains the normal ECG signal of a heart.
      abnormal : pd.DataFrame
        Dataframe that contains the abnormal ECG signal of a heart.
      time_points : list[int]
        The time points that might be visually an informative feature.

    Returns:
      Graphs two box plots. One for
    """

    # Create a df that containing the time points we want to visualize and the
    # values of those time points
    df_normal_melt = normal.melt(
        value_vars=time_points, var_name="Time Point", value_name="Amplitudes"
    )
    df_abnormal_melt = abnormal.melt(
        value_vars=time_points, var_name="Time Point", value_name="Amplitudes"
    )

    fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(14, 8))

    # Plotting normal first
    sns.boxenplot(
        data=df_normal_melt,
        x="Time Point",
        y="Amplitudes",
        dodge=True,
        linewidth=1.5,
        ax=axes[0],
    )
    axes[0].set_title(
        "Normal Category ECG Amplitude Distribution by Class at Selected Time Points",
        fontsize=10,  # Slightly smaller than a main title usually
        fontweight="bold",
        pad=15,
    )
    axes[0].set_xlabel("Time Point (ms)", fontsize=8)
    axes[0].set_ylabel("Amplitude (mV)", fontsize=8)

    # Plotting abnormal second
    sns.boxenplot(
        data=df_abnormal_melt,
        x="Time Point",
        y="Amplitudes",
        dodge=True,
        linewidth=1.5,
        ax=axes[1],
        color="#2ecc71",
    )
    axes[1].set_title(
        "Abnormal Category ECG Amplitude Distribution by Class at Selected Time Points",
        fontsize=10,
        fontweight="bold",
        pad=15,
    )
    axes[1].set_xlabel("Time Point (ms)", fontsize=8)
    axes[1].set_ylabel("Amplitude (mV)", fontsize=8)
    plt.tight_layout()
    plt.show()


visualize_informative_regions(
    normal=normal_df, abnormal=abnormal_df, time_points=[25, 50, 100, 125]
)











# Removing the classifications from the abnormal and normal dfs.
normal_X = normal_df.iloc[:, :-1].values
normal_y = normal_df[LABEL_COL]
abnormal_X = abnormal_df.iloc[:, :-1].values
abnormal_y = abnormal_df[LABEL_COL]
# The threshold is used to see how many times is a value
# greater or less than the average. The threshold is 9
# because the QRS wave will make it seem that every
# signal is an outlier because the QRS wave makes
# everything an outlier.
threshold = 9
# axis = 1 means that we are doing it cols by cols
z_scores_normal = np.abs(zscore(normal_X, axis=1))

outlier_normal_rows = np.where(np.max(z_scores_normal, axis=1) > threshold)[0]
if len(outlier_normal_rows) > 0:
    print(
        f"Normal Dataset: {len(outlier_normal_rows)} potential outlier signals (Z-score > {threshold})."
    )

# Abnormal
z_scores_abnormal = np.abs(zscore(abnormal_X, axis=1))
# This finds the rows where the z-score is
outlier_abnormal_rows = np.where(np.max(z_scores_abnormal, axis=1) > threshold)[0]
if len(outlier_abnormal_rows) > 0:
    print(
        f"Abnormal Dataset: {len(outlier_abnormal_rows)} potential outlier signals (Z-score > {threshold})."
    )











plt.figure(figsize=(15, 8))
# We want the mean of each time step(i.e., column)
# so we do axis=0.
normal_signal_avg = normal_X.mean(axis=0)
abnormal_signal_avg = abnormal_X.mean(axis=0)
plt.plot(normal_signal_avg, label=f"Mean of Normal Heartbeat")
plt.plot(abnormal_signal_avg, label=f"Mean of Abnormal Heartbeat")
plt.title("Mean ECG Normal and Abnormal Heartbeat Signals. ", fontsize=16)
plt.xlabel("Time (samples)")
plt.ylabel("Mean Amplitude")
plt.legend()
plt.show()
normal_signal_avg.shape


normal_signal_avg = normal_X.mean(axis=1)
abnormal_signal_avg = abnormal_X.mean(axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    normal_signal_avg, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    abnormal_signal_avg,
    color="red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)

plt.title("Distribution of Means: Normal vs Abnormal")
plt.xlabel("Means")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()








plt.figure(figsize=(15, 8))
# We want the mean of each time step(i.e., column)
# so we do axis=0.
normal_signal_var = normal_X.var(axis=0)
abnormal_signal_var = abnormal_X.var(axis=0)
plt.plot(normal_signal_var, label=f"Variance of Normal Heartbeat")
plt.plot(abnormal_signal_var, label=f"Variance of Abnormal Heartbeat")
plt.title("Variance ECG Normal and Abnormal Heartbeat Signals. ", fontsize=16)
plt.xlabel("Time (samples)")
plt.ylabel("Variance Amplitude")
plt.legend()
plt.show()


normal_signal_var = normal_X.var(axis=1)
abnormal_signal_var = abnormal_X.var(axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    normal_signal_var, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    abnormal_signal_var,
    color="red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)

plt.title("Distribution of Variance: Normal vs Abnormal")
plt.xlabel("Variance")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()





skewness_normal = skew(normal_X, axis=1)
skewness_abnormal = skew(abnormal_X, axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    skewness_normal, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    skewness_abnormal,
    color="Red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)
plt.title("Distribution of Signal Skewness: Normal")
plt.xlabel(
    "Skewness Value (Negative = Waveform goes down, Positive = Waveform goes up)"
)
plt.ylabel("Density (Frequency)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()





labels = ["normal", "abnormal"]
normal_kurt = kurtosis(normal_X, axis=1)
abnormal_kurt = kurtosis(abnormal_X, axis=1)
kurt_plots = [normal_kurt, abnormal_kurt]
colors = ["peachpuff", "tomato"]
fig, ax = plt.subplots(figsize=(10, 6))
ax.set_ylabel("Kurtosis")
bplot = ax.boxplot(kurt_plots, patch_artist=True, tick_labels=labels)
for patch, color in zip(bplot["boxes"], colors):
    patch.set_facecolor(color)

plt.show()
normal_kurt.mean(), abnormal_kurt.mean()
normal_kurt.shape, abnormal_kurt.shape


plt.figure(figsize=(10, 6))
sns.histplot(
    normal_kurt, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    abnormal_kurt,
    color="Red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)
plt.title("Distribution of Signal Kurtosis: Normal vs Abnormal")
plt.xlabel(
    "Skewness Value (Negative = Waveform goes down, Positive = Waveform goes up)"
)
plt.ylabel("Density (Frequency)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()





normal_median = np.median(normal_X, axis=1)
abnormal_median = np.median(abnormal_X, axis=1)
median_plots = [normal_median, abnormal_median]
colors = ["peachpuff", "tomato"]
fig, ax = plt.subplots(figsize=(10, 6))
ax.set_ylabel("Median")
bplot = ax.boxplot(median_plots, patch_artist=True, tick_labels=labels)
for patch, color in zip(bplot["boxes"], colors):
    patch.set_facecolor(color)

plt.show()
normal_median.shape, abnormal_median.shape


plt.figure(figsize=(10, 6))
sns.histplot(
    normal_median, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    abnormal_median,
    color="Red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)
plt.title("Distribution of Signal Median: Normal Vs Abnormal.")
plt.xlabel("Median Values")
plt.ylabel("Density (Frequency)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()


abnormal_median.mean(), normal_median.mean()








T = 1 / FS
N = len(normal_X[0])

# FFT
yf_normal = rfft(normal_X, axis=-1)
yf_abnormal = rfft(abnormal_X, axis=-1)
xf = rfftfreq(N, T)
yf_normal.shape, yf_abnormal.shape, xf.shape


# Nyquist Frequency
nq = FS // 2
yf_normal = yf_normal
yf_abnormal = yf_abnormal

xf.shape, yf_normal.shape, yf_abnormal.shape


band_mask = (xf >= 0) & (xf <= 10)


choose_rand_signal = np.random.randint(0, len(yf_normal))
choose_rand_signal


plt.figure(figsize=(15, 8))
choose_rand_signal = np.random.randint(0, len(yf_normal))
plt.plot(xf, np.abs(yf_normal[choose_rand_signal] ** 2), label=f"Normal")
plt.plot(xf, np.abs(yf_abnormal[choose_rand_signal] ** 2), label=f"Abnormal")
plt.title("Frequency Analysis", fontsize=16)
plt.xlabel("Freq")
plt.ylabel("Amp")
plt.legend()
plt.show()


# Trying to calculate spectral centroid
# https://stackoverflow.com/questions/54032515/spectral-centroid-of-numpy-array
mag_normal = np.abs(yf_normal)

top_normal = np.sum(mag_normal * xf, axis=-1)
bottom_normal = np.sum(mag_normal, axis=-1)
# we do not want to divide by 0
bottom_normal[bottom_normal == 0] = 1e-10
top_normal.shape, bottom_normal.shape


sc = top_normal / bottom_normal
sc


def calculate_band_power(
    signals: np.ndarray, low_freq: int, high_freq: int, fs: int = FS
) -> np.ndarray:
    """Converts multiple signals into their frequency domain. Then
    calculates the sum of FFT magnitudes within a specified frequency band.

    This function isolates the frequencies between `low_freq` (inclusive)
    and `high_freq` (inclusive) and sums the corresponding values from the
    FFT array.

    Args:
      signals : np.ndarray
        2D array of raw signals.
      fs : int
        Sample rate.
      low_freq : int
        The lower bound of the frequency band (inclusive).
      high_freq : int
        The upper bound of the frequency band (inclusive).
    Returns:
        The sum of values within the band.

    """
    period = 1 / fs
    num_of_samples = signals.shape[-1]
    freqs = rfftfreq(num_of_samples, period)
    # FFT to get the magnitudes of the signals
    mag = np.abs(rfft(signals, axis=-1))
    # low <= f < high
    band_mask = (freqs >= low_freq) & (freqs < high_freq)

    mag_band = mag[..., band_mask]
    return np.sum(mag_band, axis=-1)





def spectral_centroid(signals: np.ndarray, fs: int = FS) -> np.ndarray:
    """Takes raw signals and tries to find the spectral centroid.
    The spectral centroid indicates if the frequencies are high,
    if high, then the spectral centroid is high for that signal.

    Args:
      signals : np.ndarray
        2D array of raw signals.
      fs : int
        Sample rate.

    Returns:
      1D array corrosponding to each row of the original array of signals.
      Each index has the spectral centroid value for that signal.

    """
    # NOTE(s1perera) Aug 19, 2025: possible use welch instead because we reduce the noise of the signal
    period = 1 / fs
    num_of_samples = signals.shape[-1]
    xf = rfftfreq(num_of_samples, period)
    # FFT to get the magnitudes of the signals
    mag = np.abs(rfft(signals, axis=-1))
    # https://stackoverflow.com/questions/54032515/spectral-centroid-of-numpy-array
    top = np.sum(mag * xf, axis=-1)
    bottom = np.sum(mag, axis=-1)
    bottom[bottom == 0] = 1e-10
    sc = top / bottom
    return sc





X = df.iloc[:, :-1].values
y = df[LABEL_COL]
X.shape


def extract_features(signals: np.ndarray) -> pd.DataFrame:
    """Extracts all the statistical and physical features from each signal.

    This is a dimentionality reduction process to reduce the number
    of feature we currently have.

    Args:
      signals : np.ndarray
        2D array of signals. Where each row is a signaland each column is a timestep.

    Returns:
      Features of the signal within a dataframe
    """
    # Statistical Moments
    means = signals.mean(axis=1)
    std = signals.std(axis=1)
    median = np.median(signals, axis=1)
    # Possible physical stats
    energy = np.sum(signals**2, axis=1)
    rPeak = np.max(signals, axis=1)
    minValue = np.min(signals, axis=1)

    slope_avg = np.mean(np.diff(signals), axis=1)
    zero_crossing = np.sum(
        np.diff(
            (np.sign(signals - (signals.mean(axis=1, keepdims=True))) >= 0), axis=1
        ),
        axis=1,
    )
    sc = spectral_centroid(signals=signals)
    l = 0
    h = 10
    band_ten_to_twenty_five = calculate_band_power(
        signals=signals, low_freq=l, high_freq=h
    )

    df = pd.DataFrame(
        {
            "mean": means,
            "stdDev": std,
            "median": median,
            "energy": energy,
            "rPeak": rPeak,
            "minValue": minValue,
            "slope_avg": slope_avg,
            "zeroCrossing": zero_crossing,
            "spectralCentroid": sc,
            f"band{l}to{h}Freq": band_ten_to_twenty_five,
        }
    )

    return df


def extract_features_catch22(signals: np.ndarray) -> pd.DataFrame:
    """
    Extracts statistical, physical, and Catch22 features from each signal.

    This is a dimentionality reduction process to reduce the number
    of feature we currently have.

    Args:
      signals : np.ndarray
        2D array of signals. Where each row is a signaland each column is a timestep.

    Returns:
      Features of the signal within a dataframe
    """
    # Original features
    means = signals.mean(axis=1)
    std = signals.std(axis=1)
    median = np.median(signals, axis=1)
    energy = np.sum(signals**2, axis=1)
    rPeak = np.max(signals, axis=1)
    minValue = np.min(signals, axis=1)
    slope_avg = np.mean(np.diff(signals), axis=1)

    # np.newaxis and  keepdims=True, this way we are using the means we created
    # and not creating a new set of means in memory. just more efficient.
    mean_centered = signals - means[:, np.newaxis]
    zero_crossing = np.sum(np.diff(np.sign(mean_centered) >= 0, axis=1), axis=1)

    # Frequency analysis
    sc = spectral_centroid(signals=signals)
    l, h = 0, 10
    band_power = calculate_band_power(signals=signals, low_freq=l, high_freq=h)

    # Using Catch22 to get features
    # We iterate through rows and collect the 22 features for each
    c22_list = []
    for row in signals:
        # catch24=True includes mean/std, but since you have them manually,
        # we can stick to the 22 canonical features.
        res = pycatch22.catch22_all(row, short_names=True)
        c22_list.append(res["values"])

    # Get column names from the last successful extraction
    c22_colnames = res["short_names"]
    c22_df = pd.DataFrame(c22_list, columns=c22_colnames)

    # Now we combine everything
    manual_features = pd.DataFrame(
        {
            "mean": means,
            "stdDev": std,
            "median": median,
            "energy": energy,
            "rPeak": rPeak,
            "minValue": minValue,
            "slope_avg": slope_avg,
            "zeroCrossing": zero_crossing,
            "spectralCentroid": sc,
            f"band{l}to{h}Freq": band_power,
        }
    )

    # Concatenate the manual features with the Catch22 dataframe
    full_df = pd.concat([manual_features, c22_df], axis=1)

    return full_df


feature_df = extract_features(signals=X)
new_feature_df = extract_features_catch22(signals=X)
feature_df.shape, new_feature_df.shape


corr_matrix = feature_df.corr()
corr_matrix_extra = new_feature_df.corr()


plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.show()


plt.figure(figsize=(30, 30))
sns.heatmap(corr_matrix_extra, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.show()





X_new = new_feature_df
X_train, X_test, y_train, y_test = train_test_split(
    X_new, y, test_size=0.2, random_state=42
)
print(f"X_Train features shape: {X_train.shape}")
print(f"Y_Train target shape: {y_train.shape}")
print(f"X_Test feature shape: {X_test.shape}")
print(f"Y_Test target shape: {y_test.shape}")


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)


# n_estimator = 10 trees to be built
rfc_model = RandomForestClassifier(n_estimators=11)
# chose 6 feature because of the correlation matrix
rfe_rfc = RFE(rfc_model, n_features_to_select=33)
rfe_rfc.fit(X_train_scaled, y_train)


print(f"Feature mask: {rfe_rfc.support_}")
print(f"All columns: {X_train.columns}")
selected_features = X_train.columns[rfe_rfc.support_].tolist()
print(f"Selected Features: {selected_features}")


X_training = X_new.copy()
X_training = X_training[selected_features]
X_training.head()








X_train, X_test, y_train, y_test = train_test_split(
    X_training, y, test_size=0.2, random_state=42
)
print(f"X_Train features shape: {X_train.shape}")
print(f"Y_Train target shape: {y_train.shape}")
print(f"X_Test feature shape: {X_test.shape}")
print(f"Y_Test target shape: {y_test.shape}")


scaler = StandardScaler()
X_train_scaler = scaler.fit_transform(X_train)
X_test_scaler = scaler.fit_transform(X_test)


clf = DecisionTreeClassifier(criterion="entropy", max_depth=5, random_state=42)
clf.fit(X_train_scaler, y_train)


y_pred = clf.predict(X_test_scaler)


print(classification_report(y_true=y_test, y_pred=y_pred))
print("#" * 70)
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_pred=y_pred, y_true=y_test)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Score")
plt.ylabel("Actual Score")
plt.show()





feature_names = X_training.columns
class_names = [str(c) for c in [0, 1]]
plt.figure(figsize=(40, 20))
plot_tree(
    clf,
    feature_names=feature_names,
    class_names=class_names,
    filled=True,
    rounded=True,
    fontsize=12,
)

plt.show()








sm = SMOTE(random_state=42)  # 42 is the answer to life
X_train_scaled_upSMOTE, y_train_scaled_upSMOTE = sm.fit_resample(X_train, y_train)
# Checking how many sample we produce
print(f"Count of score '0' - Before Oversampling: {sum(y_train == 0)}")
print(f"Count of score '0' - After Oversampling: {sum(y_train_scaled_upSMOTE == 0)}")
print(f"Count of score '1' - Before Oversampling: {sum(y_train == 1)}")
print(f"Count of score '1' - After Oversampling: {sum(y_train_scaled_upSMOTE == 1)}")


X_train_smote = scaler.fit_transform(X_train_scaled_upSMOTE)


clf_smote = DecisionTreeClassifier(criterion="entropy", max_depth=5, random_state=42)
clf_smote.fit(X_train_smote, y_train_scaled_upSMOTE)


y_pred_smote = clf_smote.predict(X_test_scaler)


print(classification_report(y_true=y_test, y_pred=y_pred_smote))
print("#" * 70)
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_pred=y_pred_smote, y_true=y_test)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Score")
plt.ylabel("Actual Score")
plt.show()









