





import random

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pycatch22
import seaborn as sns
from imblearn.over_sampling import SMOTE

# Scipy related imports
from scipy.fft import rfft, rfftfreq
from scipy.signal import find_peaks
from scipy.stats import kurtosis, skew, zscore

# Sklearn related imports
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.manifold import TSNE
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree





SAMPLE_RATE = 125
CLASSIFICATION_COL = 187
RANDOM_SEED = 42








# Setting the paths of where I downloaded the datasets.
normal_train_path = "./.data/ptbdb_normal.csv"
abnormal_train_path = "./.data/ptbdb_abnormal.csv"
normal_df = pd.read_csv(normal_train_path, header=None)
abnormal_df = pd.read_csv(abnormal_train_path, header=None)
df = pd.concat([normal_df, abnormal_df], axis=0)
df.head()


classes_freq = df.iloc[:, -1].value_counts()
sorted_classes_freq = classes_freq.sort_index()
sorted_classes_freq


assert df.isna().sum().sum() == 0
print("No missing values in data!")


df.dtypes


df = df.astype("float32")
df.dtypes


# Getting the counts and the way to label the plot with percentage
counts = df[CLASSIFICATION_COL].value_counts()
percentage = counts / len(df) * 100

fig, ax = plt.subplots(figsize=(10, 8))
bars = ax.bar(["Abnormal", "Normal"], counts, color=["#4e79a7", "#f28e2b"])
labels = [f"{pct:.1f}%" for pct in percentage]

# Adding the labels
ax.bar_label(bars, labels=labels, padding=3, fontsize=12, fontweight="bold")
ax.set_title("Classification Distribution", fontsize=14, pad=20)
ax.set_ylabel("Frequency (Count)")
ax.set_ylim(0, max(counts) * 1.15)  # Add headroom for labels
plt.show()








def visualize_differences_with_line_graph(
    normal: pd.DataFrame, abnormal: pd.DataFrame
) -> None:
    """Visualize differences within group and between groups.

    The signals are chosen at random.

    Args:
      normal : pd.DataFrame
        Dataframe that contains the normal ECG signal of a heart.
      abnormal : pd.DataFrame
        Dataframe that contains the abnormal ECG signal of a heart.

    Returns:
     Two plots, one for abnormal and the other for normal. Each
     containing two signals.
    """
    two_rand_numbers = 2
    # Tell us which signal we are choosing
    first_within_signal = 0
    second_within_signal = 1
    first_between_signal = 0
    second_betwee_signal = 1

    # Randomly chooses a signal
    choose_rand_normal_signal = np.random.randint(0, len(normal), two_rand_numbers)
    choose_rand_abnormal_signal = np.random.randint(0, len(abnormal), two_rand_numbers)
    # Plotting
    first_plot = 1
    second_plot = 2
    fig, axes = plt.subplots(first_plot, second_plot, sharey=True, figsize=(10, 4))

    # Normal signal plotting
    axes[first_between_signal].plot(
        normal.iloc[choose_rand_normal_signal[first_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_normal_signal[first_within_signal]} ",
    )
    # Second normal signal
    axes[first_between_signal].plot(
        normal.iloc[choose_rand_normal_signal[second_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_normal_signal[second_within_signal]} ",
    )
    axes[first_between_signal].legend(
        shadow=True, frameon=True, facecolor="inherit", loc=1, fontsize=9
    )
    axes[first_between_signal].set_title("Normal")

    # Only need to set one y-axis title because they both have the same titles
    axes[first_between_signal].set_ylabel("Amplitude (mV)")

    # Abnormal Signal plotting
    axes[second_betwee_signal].plot(
        abnormal.iloc[choose_rand_abnormal_signal[first_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_abnormal_signal[first_within_signal]} ",
    )
    # Second abnormal signal
    axes[second_betwee_signal].plot(
        abnormal.iloc[choose_rand_abnormal_signal[second_within_signal], :-1].values,
        label=f"Sample Number: {choose_rand_abnormal_signal[second_within_signal]} ",
    )
    axes[second_betwee_signal].legend(
        frameon=True, facecolor="inherit", loc=1, fontsize=9
    )
    axes[second_betwee_signal].set_title("Abnormal")
    plt.tight_layout()
    plt.show()


visualize_differences_with_line_graph(abnormal=abnormal_df, normal=normal_df)








# Removing the classifications from the abnormal and normal dfs.
# Where_to_apply_z_score=0 is every time step, and Where_to_apply_z_score=1 is every signal
Where_to_apply_z_score = 1
normal_X = normal_df.iloc[:, :-1].values
normal_y = normal_df[CLASSIFICATION_COL]
abnormal_X = abnormal_df.iloc[:, :-1].values
abnormal_y = abnormal_df[CLASSIFICATION_COL]
# The threshold is used to see how many times is a value
# greater or less than the average. The threshold is 9
# because the QRS wave will make it seem that every
# signal is an outlier because the QRS wave makes
# everything an outlier.
threshold = 9
# axis = 1 means that we are doing it cols by cols
z_scores_normal = np.abs(zscore(normal_X, axis=Where_to_apply_z_score))

outlier_normal_rows = np.where(np.max(z_scores_normal, axis=1) > threshold)[0]
if len(outlier_normal_rows) > 0:
    print(
        f"Normal Dataset: {len(outlier_normal_rows)} potential outlier signals (Z-score > {threshold})."
    )
else:
    print("No outliers for Normal!")
# Abnormal
z_scores_abnormal = np.abs(zscore(abnormal_X, axis=Where_to_apply_z_score))
# This finds the rows where the z-score is
outlier_abnormal_rows = np.where(np.max(z_scores_abnormal, axis=1) > threshold)[0]
if len(outlier_abnormal_rows) > 0:
    print(
        f"Abnormal Dataset: {len(outlier_abnormal_rows)} potential outlier signals (Z-score > {threshold})."
    )
else:
    print("No outliers for Abnormal!")











plt.figure(figsize=(15, 8))
# We want the mean of each time step(i.e., column)
# so we do axis=0.
normal_signal_avg = normal_X.mean(axis=0)
abnormal_signal_avg = abnormal_X.mean(axis=0)
plt.plot(normal_signal_avg, label=f"Mean of Normal Heartbeat")
plt.plot(abnormal_signal_avg, label=f"Mean of Abnormal Heartbeat")
plt.title("Mean ECG Normal and Abnormal Heartbeat Signals. ", fontsize=16)
plt.xlabel("Time (samples)")
plt.ylabel("Mean Amplitude")
plt.legend()
plt.show()
normal_signal_avg.shape








normal_signal_avg = normal_X.std(axis=1)
abnormal_signal_avg = abnormal_X.std(axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    normal_signal_avg, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    abnormal_signal_avg,
    color="red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)

plt.title("Distribution of Standard Deviation: Normal vs Abnormal")
plt.xlabel("Standard Deviation")
plt.legend()
plt.grid(True, alpha=0.5)
plt.show()








skewness_normal = skew(normal_X, axis=1)
skewness_abnormal = skew(abnormal_X, axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    skewness_normal, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    skewness_abnormal,
    color="Red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)
plt.title("Distribution of Signal Skewness: Normal")
plt.xlabel("Skewness")
plt.ylabel("Density (Frequency)")
plt.legend()
plt.grid(True, alpha=0.5)
plt.show()








kurtosis_normal = kurtosis(normal_X, axis=1)
kurtosis_abnormal = kurtosis(abnormal_X, axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    kurtosis_normal, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    kurtosis_abnormal,
    color="Red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)
plt.title("Distribution of Signal kurtosis: Normal vs Abnormal")
plt.xlabel("kurtosis")
plt.ylabel("Density (Frequency)")
plt.legend()
plt.grid(True, alpha=0.5)
plt.show()








normal_median = np.median(normal_X, axis=1)
abnormal_median = np.median(abnormal_X, axis=1)
plt.figure(figsize=(10, 6))
sns.histplot(
    normal_median, color="blue", kde=True, label="Normal", stat="density", alpha=0.5
)
sns.histplot(
    abnormal_median,
    color="Red",
    kde=True,
    label="Abnormal",
    stat="density",
    alpha=0.5,
)
plt.title("Distribution of Signal Median: Normal Vs Abnormal.")
plt.xlabel("Median Values")
plt.ylabel("Density (Frequency)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()








T = 1 / SAMPLE_RATE
N = len(normal_X[0])

# FFT
yf_normal = rfft(normal_X, axis=-1)
yf_abnormal = rfft(abnormal_X, axis=-1)
xf = rfftfreq(N, T)

band_mask = (xf >= 0) & (xf <= 10)
choose_rand_signal = np.random.randint(0, len(yf_normal))
plt.figure(figsize=(15, 8))
choose_rand_signal = np.random.randint(0, len(yf_normal))
plt.plot(xf, np.abs(yf_normal[choose_rand_signal] ** 2), label=f"Normal")
plt.plot(xf, np.abs(yf_abnormal[choose_rand_signal] ** 2), label=f"Abnormal")
plt.title("Frequency Analysis", fontsize=16)
plt.xlabel("Freq")
plt.ylabel("Amp")
plt.legend()
plt.show()








def calculate_band_power(
    signals: np.ndarray, low_freq: int, high_freq: int, fs: int = SAMPLE_RATE
) -> np.ndarray:
    """Converts multiple signals into their frequency domain. Then
    calculates the sum of FFT magnitudes within a specified frequency band.

    This function isolates the frequencies between `low_freq` (inclusive)
    and `high_freq` (inclusive) and sums the corresponding values from the
    FFT array.

    Args:
      signals : np.ndarray
        2D array of raw signals.
      fs : int
        Sample rate.
      low_freq : int
        The lower bound of the frequency band (inclusive).
      high_freq : int
        The upper bound of the frequency band (inclusive).
    Returns:
        The sum of values within the band.

    """
    period = 1 / fs
    num_of_samples = signals.shape[-1]
    freqs = rfftfreq(num_of_samples, period)
    # FFT to get the magnitudes of the signals
    mag = np.abs(rfft(signals, axis=-1))
    # low <= f < high
    band_mask = (freqs >= low_freq) & (freqs < high_freq)

    mag_band = mag[..., band_mask]
    return np.sum(mag_band, axis=-1)


def spectral_centroid(signals: np.ndarray, fs: int = SAMPLE_RATE) -> np.ndarray:
    """Takes raw signals and tries to find the spectral centroid.
    The spectral centroid indicates if the frequencies are high,
    if high, then the spectral centroid is high for that signal.

    Args:
      signals : np.ndarray
        2D array of raw signals.
      fs : int
        Sample rate.

    Returns:
      1D array corrosponding to each row of the original array of signals.
      Each index has the spectral centroid value for that signal.

    """
    # NOTE(s1perera) Aug 19, 2025: possible use welch instead because we reduce the noise of the signal
    period = 1 / fs
    num_of_samples = signals.shape[-1]
    xf = rfftfreq(num_of_samples, period)
    # FFT to get the magnitudes of the signals
    mag = np.abs(rfft(signals, axis=-1))
    # https://stackoverflow.com/questions/54032515/spectral-centroid-of-numpy-array
    top = np.sum(mag * xf, axis=-1)
    bottom = np.sum(mag, axis=-1)
    bottom[bottom == 0] = 1e-10
    sc = top / bottom
    return sc


def extract_features(signals: np.ndarray) -> pd.DataFrame:
    """Extracts all the statistical and physical features from each signal.

    This is a dimentionality reduction process to reduce the number
    of feature we currently have.

    Args:
      signals : np.ndarray
        2D array of signals. Where each row is a signaland each column is a timestep.

    Returns:
      Features of the signal within a dataframe
    """
    # Statistical Moments
    means = signals.mean(axis=1)
    std = signals.std(axis=1)
    median = np.median(signals, axis=1)
    # Possible physical stats
    energy = np.sum(signals**2, axis=1)
    rPeak = np.max(signals, axis=1)
    minValue = np.min(signals, axis=1)
    print(rPeak)

    slope_avg = np.mean(np.diff(signals), axis=1)
    zero_crossing = np.sum(
        np.diff(
            (np.sign(signals - (signals.mean(axis=1, keepdims=True))) >= 0), axis=1
        ),
        axis=1,
    )
    sc = spectral_centroid(signals=signals)
    l = 0
    h = 10
    c = 10
    d = 20
    band_one = calculate_band_power(signals=signals, low_freq=l, high_freq=h)
    band_two = calculate_band_power(signals=signals, low_freq=c, high_freq=d)

    df = pd.DataFrame(
        {
            "mean": means,
            "stdDev": std,
            "median": median,
            "energy": energy,
            "rPeak": rPeak,
            "minValue": minValue,
            "slope_avg": slope_avg,
            "zeroCrossing": zero_crossing,
            "spectralCentroid": sc,
            f"band{l}to{h}Freq": band_one,
            f"band{c}to{d}Freq": band_two,
        }
    )

    return df


def extract_features_catch22(signals: np.ndarray) -> pd.DataFrame:
    """
    Extracts statistical, physical, and Catch22 features from each signal.

    This is a dimentionality reduction process to reduce the number
    of feature we currently have.

    Args:
      signals : np.ndarray
        2D array of signals. Where each row is a signal and each column is a timestep.

    Returns:
      Features of the signal within a dataframe
    """
    # Original features
    means = signals.mean(axis=1)
    std = signals.std(axis=1)
    median = np.median(signals, axis=1)
    energy = np.sum(signals**2, axis=1)
    rPeak = np.max(signals, axis=1)
    minValue = np.min(signals, axis=1)
    slope_avg = np.mean(np.diff(signals), axis=1)

    # np.newaxis and  keepdims=True, this way we are using the means we created
    # and not creating a new set of means in memory. just more efficient.
    mean_centered = signals - means[:, np.newaxis]
    zero_crossing = np.sum(np.diff(np.sign(mean_centered) >= 0, axis=1), axis=1)

    # Frequency analysis
    sc = spectral_centroid(signals=signals)
    l, h = 0, 10
    band_power = calculate_band_power(signals=signals, low_freq=l, high_freq=h)

    # Using Catch22 to get features
    # We iterate through rows and collect the 22 features for each
    c22_list = []
    for row in signals:
        # catch24=True includes mean/std, but since you have them manually,
        # we can stick to the 22 canonical features.
        res = pycatch22.catch22_all(row, short_names=True)
        c22_list.append(res["values"])

    # Get column names from the last successful extraction
    c22_colnames = res["short_names"]
    c22_df = pd.DataFrame(c22_list, columns=c22_colnames)

    # Now we combine everything
    manual_features = pd.DataFrame(
        {
            "mean": means,
            "stdDev": std,
            "median": median,
            "energy": energy,
            "rPeak": rPeak,
            "minValue": minValue,
            "slope_avg": slope_avg,
            "zeroCrossing": zero_crossing,
            "spectralCentroid": sc,
            f"band{l}to{h}Freq": band_power,
        }
    )

    # Concatenate the manual features with the Catch22 dataframe
    full_df = pd.concat([manual_features, c22_df], axis=1)

    return full_df


# First we split the data into data and the classification.
X = df.iloc[:, :-1].values
y = df[CLASSIFICATION_COL]
X.shape


# Then we grab the features
feature_df = extract_features(signals=X)
catch22_feature_df = extract_features_catch22(signals=X)
feature_df.shape, catch22_feature_df.shape


corr_matrix = feature_df.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.show()


corr_matrix_catch22 = catch22_feature_df.corr()
plt.figure(figsize=(30, 30))
sns.heatmap(corr_matrix_catch22, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.show()


def get_high_correlation_features(
    corr_matrix: pd.DataFrame, p_value_greater_than: float = 0.7
) -> set[str]:
    """Identifies unique features with a correlation above a specified threshold.

    This function masks the upper triangle of the correlation matrix to avoid
    redundant pairs (A-B and B-A) and self-correlation (A-A), filters for
    high values, and extracts the unique feature names involved.

    Args:
        corr_matrix : pd.DataFrame
          A square pandas DataFrame representing the correlation matrix.
        p_value_greater_than : float
          The absolute p-value coefficient cutoff.

    Returns:
        A set of feature names that have at least one high-correlation connection.
    """
    # Create a mask for the upper triangle (not including the diagonal, this is the self correlation)
    # This prevents counting (A, B) and (B, A) as two different pairs and counting pairs like (A,A)
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    # Apply mask, unstack to 1D pairs, and drop nans/lower triangle of correlation matrix
    unique_pairs = corr_matrix.mask(mask).unstack().dropna()

    # Filter for pairs with an absolute correlation higher than the threshold
    high_corr = unique_pairs[unique_pairs.abs() >= p_value_greater_than]

    # Extract unique feature names using set union of both index levels
    unique_names = set(high_corr.index.get_level_values(0)) | set(
        high_corr.index.get_level_values(1)
    )

    # Print statements for verification
    print(high_corr.sort_values(ascending=False))
    print("The number of features you want is: ", len(unique_names))
    print(unique_names)

    return unique_names


top_corr_features = get_high_correlation_features(
    corr_matrix=corr_matrix_catch22, p_value_greater_than=0.7
)


def best_feature_with_RFE(
    X: pd.DataFrame,
    y: pd.Series,
    n_features_to_select: int,
    test_size: float = 0.2,
    random_state: int = 42,
) -> list:
    """Selects the most impactful features from a dataset using Recursive Feature Elimination (RFE)
    with a Random Forest Classifier.

    This function performs a train-test split, scales the feature data using StandardScaler,
    and applies RFE to iteratively remove the least important features until the specified
    number of features is reached.

    Args:
      X : pd.DataFrame
        A DataFrame containing all the features extracted from a 2D array of signals.
      y : pd.Series
        The classification of the signals.
      n_features_to_select : int
        The top total number of features to select.
      test_size : float
        The proportion of the dataset to include in the test split.
        Defaults to 20%(input value is equivalent to 0.2).
      random_state : int
        gives randomness to RFE.


    Returns:
      A list of strings containing the names of the selected feature columns.
    """
    # Training split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    print(f"X_Train features shape: {X_train.shape}")
    print(f"Y_Train target shape: {y_train.shape}")
    print(f"X_Test feature shape: {X_test.shape}")
    print(f"Y_Test target shape: {y_test.shape}")
    # Scale so no feature has more importance
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    # Initialize Random Forest
    # NOTE(shenuk) Dec 29, 2025: Need to explain n_estimators
    rfc_model = RandomForestClassifier(
        n_estimators=11, random_state=random_state, n_jobs=-1
    )

    # Initialize RFE to select the top N features
    rfe_rfc = RFE(estimator=rfc_model, n_features_to_select=n_features_to_select)
    rfe_rfc.fit(X_train_scaled, y_train)

    # Printing the extracted results to verify them
    print(f"\nFeature mask: {rfe_rfc.support_}")

    # Use the boolean mask (support_) to filter the column names
    selected_features = X_train.columns[rfe_rfc.support_].tolist()

    print(f"Selected Features: {selected_features}")

    return selected_features


selected_features = best_feature_with_RFE(
    X=catch22_feature_df,
    y=y,
    n_features_to_select=len(top_corr_features),
    random_state=RANDOM_SEED,
)





def model_pipeline(
    X: pd.DataFrame,
    y: pd.Series,
    model: any,
    selected_features: list | None,
    random_seed: int = 42,
) -> None:
    """Trains a machine learning model and evaluates its performance through metrics and visualization.

    This function filters the dataset for selected features, performs a train-test split,
    scales the data, and then fits the provided model. It outputs a classification report
    and displays a confusion matrix heatmap.

    Args:
      X : pd.DataFrame
        A DataFrame containing all the features extracted from a 2D array of signals.
      y : pd.Series
        The classification of the signals.
      model : Any
        A machine learning model object (e.g., RandomForestClassifier, SVM, etc.).
      selected_features : Optional[List[str]]
        A list of specific feature column names to use for training. Defaults to None.
      random_seed : int
        random number used for reproducibility in the data split.


    Returns:
      None
    """
    # Filter features if a list is provided
    if selected_features is not None:
        X = X[selected_features]

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_seed
    )

    print(f"X_Train features shape: {X_train.shape}")
    print(f"Y_Train target shape: {y_train.shape}")
    print(f"X_Test feature shape: {X_test.shape}")
    print(f"Y_Test target shape: {y_test.shape}")

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Predicting
    model.fit(X_train_scaled, y_train)
    model_pred_y = model.predict(X_test_scaled)

    print("\nClassification Report:")
    print(classification_report(y_true=y_test, y_pred=model_pred_y))
    print("#" * 70)

    # Visualization
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_true=y_test, y_pred=model_pred_y)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Score")
    plt.ylabel("Actual Score")
    plt.title(f"Confusion Matrix: {type(model).__name__}")
    plt.show()





rfc_model = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1)
model_pipeline(
    X=catch22_feature_df, y=y, model=rfc_model, selected_features=selected_features
)





clf = DecisionTreeClassifier(criterion="entropy", max_depth=5, random_state=42)
model_pipeline(
    X=catch22_feature_df, y=y, model=clf, selected_features=selected_features
)






