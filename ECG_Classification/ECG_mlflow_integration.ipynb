{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb83989d-c870-4165-978e-8fcadc6cb341",
   "metadata": {},
   "source": [
    "### Training Jobs Tracking\n",
    "- The goal of this notebook is to experiment on how to track training jobs. Specifically to understand what needs to be tracked and how to track them. Additionally, figuring out the multitude of hyperparamter and how to log them for every training run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453eb51-a7b7-47de-97c2-4c93b353d745",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16302fb-b72c-4ed3-995b-f3931dcabad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycatch22\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Scipy related imports\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis, skew, zscore\n",
    "\n",
    "# Sklearn related imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde2f68-f990-4b2f-86b0-5a88281968eb",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b60bee9-04e3-4bc3-af55-69dd1323eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 125\n",
    "LABEL_COL = 187\n",
    "RANDOM_SEED = 42\n",
    "CLASSIFICATION_COL = 186"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea140421-6da8-4ee8-a503-ff6d3ed1ac4e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe342043-7c28-450b-ae2f-a1006cff5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_band_power(\n",
    "    signals: np.ndarray, low_freq: int, high_freq: int, fs: int = FS\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Converts multiple signals into their frequency domain. Then\n",
    "    calculates the sum of FFT magnitudes within a specified frequency band.\n",
    "\n",
    "    This function isolates the frequencies between `low_freq` (inclusive)\n",
    "    and `high_freq` (inclusive) and sums the corresponding values from the\n",
    "    FFT array.\n",
    "\n",
    "    Args:\n",
    "      signals : np.ndarray\n",
    "        2D array of raw signals.\n",
    "      fs : int\n",
    "        Sample rate.\n",
    "      low_freq : int\n",
    "        The lower bound of the frequency band (inclusive).\n",
    "      high_freq : int\n",
    "        The upper bound of the frequency band (inclusive).\n",
    "    Returns:\n",
    "        The sum of values within the band.\n",
    "\n",
    "    \"\"\"\n",
    "    period = 1 / fs\n",
    "    num_of_samples = signals.shape[-1]\n",
    "    freqs = rfftfreq(num_of_samples, period)\n",
    "    # FFT to get the magnitudes of the signals\n",
    "    mag = np.abs(rfft(signals, axis=-1))\n",
    "    # low <= f < high\n",
    "    band_mask = (freqs >= low_freq) & (freqs < high_freq)\n",
    "\n",
    "    mag_band = mag[..., band_mask]\n",
    "    return np.sum(mag_band, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a36d733-7d20-4b23-a0a3-a8a5a6bb414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_centroid(signals: np.ndarray, fs: int = FS) -> np.ndarray:\n",
    "    \"\"\"Takes raw signals and tries to find the spectral centroid.\n",
    "    The spectral centroid indicates if the frequencies are high,\n",
    "    if high, then the spectral centroid is high for that signal.\n",
    "\n",
    "    Args:\n",
    "      signals : np.ndarray\n",
    "        2D array of raw signals.\n",
    "      fs : int\n",
    "        Sample rate.\n",
    "\n",
    "    Returns:\n",
    "      1D array corrosponding to each row of the original array of signals.\n",
    "      Each index has the spectral centroid value for that signal.\n",
    "\n",
    "    \"\"\"\n",
    "    # NOTE(s1perera) Aug 19, 2025: possible use welch instead because we reduce the noise of the signal\n",
    "    period = 1 / fs\n",
    "    num_of_samples = signals.shape[-1]\n",
    "    xf = rfftfreq(num_of_samples, period)\n",
    "    # FFT to get the magnitudes of the signals\n",
    "    mag = np.abs(rfft(signals, axis=-1))\n",
    "    # https://stackoverflow.com/questions/54032515/spectral-centroid-of-numpy-array\n",
    "    top = np.sum(mag * xf, axis=-1)\n",
    "    bottom = np.sum(mag, axis=-1)\n",
    "    bottom[bottom == 0] = 1e-10\n",
    "    sc = top / bottom\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdae2b73-1c58-4c4f-ab42-1d4b4d72eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(signals: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Extracts all the statistical and physical features from each signal.\n",
    "\n",
    "    This is a dimentionality reduction process to reduce the number\n",
    "    of feature we currently have.\n",
    "\n",
    "    Args:\n",
    "      signals : np.ndarray\n",
    "        2D array of signals. Where each row is a signaland each column is a timestep.\n",
    "\n",
    "    Returns:\n",
    "      Features of the signal within a dataframe\n",
    "    \"\"\"\n",
    "    # Statistical Moments\n",
    "    means = signals.mean(axis=1)\n",
    "    std = signals.std(axis=1)\n",
    "    median = np.median(signals, axis=1)\n",
    "    # Possible physical stats\n",
    "    energy = np.sum(signals**2, axis=1)\n",
    "    rPeak = np.max(signals, axis=1)\n",
    "    minValue = np.min(signals, axis=1)\n",
    "\n",
    "    slope_avg = np.mean(np.diff(signals), axis=1)\n",
    "    zero_crossing = np.sum(\n",
    "        np.diff(\n",
    "            (np.sign(signals - (signals.mean(axis=1, keepdims=True))) >= 0), axis=1\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    sc = spectral_centroid(signals=signals)\n",
    "    l = 0\n",
    "    h = 10\n",
    "    band_ten_to_twenty_five = calculate_band_power(\n",
    "        signals=signals, low_freq=l, high_freq=h\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"mean\": means,\n",
    "            \"stdDev\": std,\n",
    "            \"median\": median,\n",
    "            \"energy\": energy,\n",
    "            \"rPeak\": rPeak,\n",
    "            \"minValue\": minValue,\n",
    "            \"slope_avg\": slope_avg,\n",
    "            \"zeroCrossing\": zero_crossing,\n",
    "            \"spectralCentroid\": sc,\n",
    "            f\"band{l}to{h}Freq\": band_ten_to_twenty_five,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a611e7-5969-4c95-86df-8dd6e0bbdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_catch22(signals: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts statistical, physical, and Catch22 features from each signal.\n",
    "\n",
    "    This is a dimentionality reduction process to reduce the number\n",
    "    of feature we currently have.\n",
    "\n",
    "    Args:\n",
    "      signals : np.ndarray\n",
    "        2D array of signals. Where each row is a signaland each column is a timestep.\n",
    "\n",
    "    Returns:\n",
    "      Features of the signal within a dataframe\n",
    "    \"\"\"\n",
    "    # Original features\n",
    "    means = signals.mean(axis=1)\n",
    "    std = signals.std(axis=1)\n",
    "    median = np.median(signals, axis=1)\n",
    "    energy = np.sum(signals**2, axis=1)\n",
    "    rPeak = np.max(signals, axis=1)\n",
    "    minValue = np.min(signals, axis=1)\n",
    "    slope_avg = np.mean(np.diff(signals), axis=1)\n",
    "\n",
    "    # np.newaxis and  keepdims=True, this way we are using the means we created\n",
    "    # and not creating a new set of means in memory. just more efficient.\n",
    "    mean_centered = signals - means[:, np.newaxis]\n",
    "    zero_crossing = np.sum(np.diff(np.sign(mean_centered) >= 0, axis=1), axis=1)\n",
    "\n",
    "    # Frequency analysis\n",
    "    sc = spectral_centroid(signals=signals)\n",
    "    l, h = 0, 10\n",
    "    band_power = calculate_band_power(signals=signals, low_freq=l, high_freq=h)\n",
    "\n",
    "    # Using Catch22 to get features\n",
    "    # We iterate through rows and collect the 22 features for each\n",
    "    c22_list = []\n",
    "    for row in signals:\n",
    "        # catch24=True includes mean/std, but since you have them manually,\n",
    "        # we can stick to the 22 canonical features.\n",
    "        res = pycatch22.catch22_all(row, short_names=True)\n",
    "        c22_list.append(res[\"values\"])\n",
    "\n",
    "    # Get column names from the last successful extraction\n",
    "    c22_colnames = res[\"short_names\"]\n",
    "    c22_df = pd.DataFrame(c22_list, columns=c22_colnames)\n",
    "\n",
    "    # Now we combine everything\n",
    "    manual_features = pd.DataFrame(\n",
    "        {\n",
    "            \"mean\": means,\n",
    "            \"stdDev\": std,\n",
    "            \"median\": median,\n",
    "            \"energy\": energy,\n",
    "            \"rPeak\": rPeak,\n",
    "            \"minValue\": minValue,\n",
    "            \"slope_avg\": slope_avg,\n",
    "            \"zeroCrossing\": zero_crossing,\n",
    "            \"spectralCentroid\": sc,\n",
    "            f\"band{l}to{h}Freq\": band_power,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Concatenate the manual features with the Catch22 dataframe\n",
    "    full_df = pd.concat([manual_features, c22_df], axis=1)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddd25b-08d1-4c94-a860-6a9187129a70",
   "metadata": {},
   "source": [
    "### Training\n",
    "1. First we load the data.\n",
    "2. From our EDA, we can deduce that we dont need to check for duplicates or missing values, so that is skipped.\n",
    "3. Preprocessing will be considered the following:\n",
    "   * Using `catch22` import, it will extract feature on top of the features that have been manually imported.\n",
    "4. Then the training will begin with 20% of the data being the test.\n",
    "5. The training data will be normalized using scaler.\n",
    "6. Best feature extraction will be used done with `RFC` and logged as a hyperparameter.\n",
    "7. A training job will be done using data augmented through SMOTE to reduces the imbalance between normal and abnormal classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c4846e-6cf8-42bb-8199-5ef5866ba149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      float32\n",
       "1      float32\n",
       "2      float32\n",
       "3      float32\n",
       "4      float32\n",
       "        ...   \n",
       "183    float32\n",
       "184    float32\n",
       "185    float32\n",
       "186    float32\n",
       "187    float32\n",
       "Length: 188, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_train_path = \"./.data/ptbdb_normal.csv\"\n",
    "abnormal_train_path = \"./.data/ptbdb_abnormal.csv\"\n",
    "normal_df = pd.read_csv(normal_train_path, header=None)\n",
    "abnormal_df = pd.read_csv(abnormal_train_path, header=None)\n",
    "df = pd.concat([normal_df, abnormal_df], axis=0)\n",
    "df = df.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf3b4e-0a84-48ae-b4c5-f15c4814af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df[LABEL_COL]\n",
    "new_feature_df = extract_features_catch22(signals=X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_new, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECG_env",
   "language": "python",
   "name": "ecg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
